{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"1zVXsqLd4HWb"},"source":["This notebook is adapted from the official PyTorch tutorial on [Optimization](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html)."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6LwuHBnJ37_7"},"source":["# Optimizing Model Parameters\n","\n","Now that we have prepared some data and built a model, it's time to train our model by optimizing its parameters on our data. \n","\n","Training a model is an iterative process; in each iteration the model makes a guess about the output, calculates\n","the error in its guess (*loss*), collects the derivatives of the error with respect to its parameters (as we saw in\n","the last tutorial), and **optimizes** these parameters using gradient descent. \n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"S1HtUfZP45K0"},"source":["## Prerequisite Code\n","We first load data and define our model as in previous tutorials. "]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1747,"status":"ok","timestamp":1670943432424,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"lwqkp7P85Lub"},"outputs":[],"source":["import torch\n","\n","torch.set_printoptions(sci_mode=False, linewidth=300)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1213,"status":"ok","timestamp":1670943449347,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"lwvIGOtO5R3u"},"outputs":[],"source":["from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","\n","X, y = fetch_california_housing(return_X_y=True)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":357,"status":"ok","timestamp":1670943474032,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"japusbEqip1k"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","\n","# Ensure each feature is roughly on the same scale\n","std_scaler = StandardScaler().fit(X_train)\n","X_train = std_scaler.transform(X_train)\n","X_test = std_scaler.transform(X_test)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":116,"status":"ok","timestamp":1670943537753,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"zs6pgSfm37_8"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader, TensorDataset\n","\n","X_train = torch.as_tensor(X_train, dtype=torch.float)\n","y_train = torch.as_tensor(y_train, dtype=torch.float)\n","X_test = torch.as_tensor(X_test, dtype=torch.float)\n","y_test = torch.as_tensor(y_test, dtype=torch.float)\n","\n","train_dataset = TensorDataset(X_train, y_train)\n","test_dataset = TensorDataset(X_test, y_test)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":478,"status":"ok","timestamp":1670943578922,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"kLo7J7q95qwG"},"outputs":[],"source":["import torch.nn as nn\n","\n","class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super(NeuralNetwork, self).__init__() \n","\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(in_features=8, out_features=16),\n","            nn.ReLU(),\n","            nn.Linear(in_features=16, out_features=16),\n","            nn.ReLU(),\n","            nn.Linear(in_features=16, out_features=1),\n","        )\n","\n","    def forward(self, x):\n","        return self.linear_relu_stack(x)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":122,"status":"ok","timestamp":1670943583222,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"7arEA9Ys537d","outputId":"9d058e90-8ded-4a7a-80ac-ffe35fa34fb8"},"outputs":[{"data":{"text/plain":["NeuralNetwork(\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=8, out_features=16, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=16, out_features=16, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=16, out_features=1, bias=True)\n","  )\n",")"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["model = NeuralNetwork()\n","model"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"X1sYceex37_9"},"source":["## Optimization Loop\n","\n","We can then train and optimize our model with an optimization loop. Each\n","iteration of the optimization loop is called an **epoch**.\n","\n","Each epoch consists of two main parts:\n"," - **The Train Loop** - iterate over the training dataset and try to converge to optimal parameters.\n"," - **The Validation/Test Loop** - iterate over the test dataset to check if model performance is improving.\n","\n","Let's familiarize ourselves with some of the concepts used in the training loop.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7h_Iotqd8c_b"},"source":["### Loss Function\n","\n","When presented with some training data, our untrained network is likely not to give the correct\n","answer. **Loss function** measures the degree of dissimilarity of obtained result to the target value,\n","and it is the loss function that we want to minimize during training. To calculate the loss we make a\n","prediction using the inputs of our given data sample and compare it against the true data label value.\n","\n","Common loss functions include [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) (Mean Square Error) for regression tasks and\n","[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) for classification tasks. A more complete list of loss functions can be found [here](https://pytorch.org/docs/stable/nn.html#loss-functions).\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":173,"status":"ok","timestamp":1670943817096,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"LBV_ewXI99OL"},"outputs":[],"source":["loss_fn = nn.MSELoss()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Bz_ha-6137_-"},"source":["### Optimizer\n","\n","Optimization is the process of adjusting model parameters to reduce model error in each training step. **Optimization algorithms** define how this process is performed. In this tutorial, we use the vanilla Stochastic Gradient Descent (SGD).\n","\n","Optimization-related tools are provided under the [torch.optim](https://pytorch.org/docs/stable/optim.html) package, including a variety of [optimizers](https://pytorch.org/docs/stable/optim.html#algorithms). Here we use the [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD) optimizer. \n","\n","Many factors can influence the choice of your optimizer, such as the nature of the problem you are trying to solve or the model architecture you designed. The [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam) optimizer is usually the go-to one for initial exploration, but it may not be the best choice for your problem at hand. In fact, some papers such as [this](https://proceedings.neurips.cc/paper/2017/file/81b3833e2504647f9d794f7d7b9bf341-Paper.pdf) have suggested that the models trained with Adam (and the like) do not generalize well as compared to that trained with SGD.\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"56DOZkluEgZz"},"source":["We initialize an optimizer by registering the model's parameters that need to be trained and passing any required (hyper)parameters such as the learning rate."]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":120,"status":"ok","timestamp":1670944225268,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"M576YIgD37_-"},"outputs":[],"source":["import torch.optim as optim\n","\n","optimizer = optim.SGD(model.parameters(), lr=1e-3)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"VJxu51c_37_-"},"source":["Inside the training loop, optimization happens in three steps:\n"," * Call ``optimizer.zero_grad()`` to reset the gradients of model parameters. Remember that PyTorch **accumulates gradients** by default, so we need to explicitly zero the gradients at each iteration.\n"," * Backpropagate the prediction loss with a call to ``loss.backward()``. PyTorch deposits the gradients of the loss w.r.t. each parameter.\n"," * Once we have our gradients, we call ``optimizer.step()`` to adjust the parameters using the gradients collected in the backward pass.\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RJXafDh537__"},"source":["\n","## Full Implementation\n","We define ``train_loop`` that loops over our optimization code, and ``test_loop`` that\n","evaluates the model's performance against our test data.\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":98,"status":"ok","timestamp":1670944868824,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"a9JYuj8c37__"},"outputs":[],"source":["def train_loop(dataloader, model, loss_fn, optimizer, device):\n","    \n","    total = len(dataloader.dataset) # total number of examples\n","    num_data_seen = 0 # number of examples seen so far\n","\n","    model.train() # Set your model to \"train\" mode\n","\n","    for batch_idx, (X, y) in enumerate(dataloader):\n","        # Send inputs and labels to device\n","        X, y = X.to(device), y.to(device)\n","\n","        # Compute prediction and loss\n","        pred = model(X).reshape(-1) # Reshape the pred of shape (N, 1) to (N,)\n","        loss = loss_fn(pred, y)\n","\n","        # Backpropagation\n","        optimizer.zero_grad() # You must zero grad before .backward!\n","        loss.backward()\n","        optimizer.step() # Take an optimization step\n","\n","        num_data_seen += X.size(0)\n","        # Print out stats every 20 batches\n","        if batch_idx % 20 == 0:\n","            loss = loss.item() # What is .item?\n","            print(f\"loss: {loss:>7f}  [{num_data_seen:>5d}/{total:>5d}]\")"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":224,"status":"ok","timestamp":1670945036714,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"N372Zt6Keu6q"},"outputs":[],"source":["def test_loop(dataloader, model, loss_fn, device): # no more optimizer\n","    total = len(dataloader.dataset)\n","    test_loss = 0\n","\n","    model.eval() # Set your model to \"eval\" mode\n","\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            # Send inputs and labels to device\n","            X, y = X.to(device), y.to(device)\n","\n","            pred = model(X).reshape(-1) # Reshape the pred of shape (N, 1) to (N,)\n","            loss = loss_fn(pred, y)\n","\n","            # no more optimization steps\n","\n","            test_loss += loss.item() * X.size(0) # because \"loss\" was averaged over the batch\n","\n","    test_loss /= total\n","    print(f\"Avg test loss: {test_loss:>8f} \\n\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5-mD3LmS37__"},"source":["We initialize the loss function and optimizer, and pass it to ``train_loop`` and ``test_loop``.\n","Feel free to increase the number of epochs to track the model's improving performance.\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5975,"status":"ok","timestamp":1670945078557,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"EeSNhIvt37__","outputId":"b264449b-d4e1-44d2-957f-48ca51d72dd6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1\n","-------------------------------\n","loss: 7.232432  [   64/16512]\n","loss: 5.759336  [ 1344/16512]\n","loss: 4.239867  [ 2624/16512]\n","loss: 5.791527  [ 3904/16512]\n","loss: 3.029928  [ 5184/16512]\n","loss: 3.515863  [ 6464/16512]\n","loss: 3.220753  [ 7744/16512]\n","loss: 2.155186  [ 9024/16512]\n","loss: 3.036534  [10304/16512]\n","loss: 3.284680  [11584/16512]\n","loss: 2.408881  [12864/16512]\n","loss: 2.077053  [14144/16512]\n","loss: 1.759101  [15424/16512]\n","Avg test loss: 1.585000 \n","\n","Epoch 2\n","-------------------------------\n","loss: 1.970925  [   64/16512]\n","loss: 1.734903  [ 1344/16512]\n","loss: 1.432903  [ 2624/16512]\n","loss: 0.856717  [ 3904/16512]\n","loss: 1.405313  [ 5184/16512]\n","loss: 1.282586  [ 6464/16512]\n","loss: 0.992226  [ 7744/16512]\n","loss: 0.755988  [ 9024/16512]\n","loss: 1.100949  [10304/16512]\n","loss: 1.080812  [11584/16512]\n","loss: 1.005567  [12864/16512]\n","loss: 0.979074  [14144/16512]\n","loss: 0.758651  [15424/16512]\n","Avg test loss: 1.004695 \n","\n","Epoch 3\n","-------------------------------\n","loss: 0.904746  [   64/16512]\n","loss: 0.736265  [ 1344/16512]\n","loss: 0.679998  [ 2624/16512]\n","loss: 0.853313  [ 3904/16512]\n","loss: 0.905072  [ 5184/16512]\n","loss: 1.213774  [ 6464/16512]\n","loss: 0.501152  [ 7744/16512]\n","loss: 0.768721  [ 9024/16512]\n","loss: 0.859437  [10304/16512]\n","loss: 0.985451  [11584/16512]\n","loss: 0.441069  [12864/16512]\n","loss: 0.994345  [14144/16512]\n","loss: 0.859780  [15424/16512]\n","Avg test loss: 0.801811 \n","\n","Epoch 4\n","-------------------------------\n","loss: 0.496791  [   64/16512]\n","loss: 0.690672  [ 1344/16512]\n","loss: 0.701941  [ 2624/16512]\n","loss: 0.929527  [ 3904/16512]\n","loss: 0.553921  [ 5184/16512]\n","loss: 1.041063  [ 6464/16512]\n","loss: 0.591183  [ 7744/16512]\n","loss: 0.748311  [ 9024/16512]\n","loss: 0.652792  [10304/16512]\n","loss: 0.646833  [11584/16512]\n","loss: 0.711966  [12864/16512]\n","loss: 0.541623  [14144/16512]\n","loss: 0.897216  [15424/16512]\n","Avg test loss: 0.718132 \n","\n","Epoch 5\n","-------------------------------\n","loss: 0.835522  [   64/16512]\n","loss: 0.684254  [ 1344/16512]\n","loss: 0.824732  [ 2624/16512]\n","loss: 0.766353  [ 3904/16512]\n","loss: 1.144713  [ 5184/16512]\n","loss: 0.926429  [ 6464/16512]\n","loss: 0.534533  [ 7744/16512]\n","loss: 0.698409  [ 9024/16512]\n","loss: 0.629671  [10304/16512]\n","loss: 0.648340  [11584/16512]\n","loss: 0.616419  [12864/16512]\n","loss: 0.825015  [14144/16512]\n","loss: 0.669075  [15424/16512]\n","Avg test loss: 0.681906 \n","\n","Epoch 6\n","-------------------------------\n","loss: 0.785570  [   64/16512]\n","loss: 0.716904  [ 1344/16512]\n","loss: 0.706423  [ 2624/16512]\n","loss: 0.671617  [ 3904/16512]\n","loss: 0.555085  [ 5184/16512]\n","loss: 0.832767  [ 6464/16512]\n","loss: 0.637000  [ 7744/16512]\n","loss: 0.423128  [ 9024/16512]\n","loss: 0.909273  [10304/16512]\n","loss: 0.634097  [11584/16512]\n","loss: 0.621931  [12864/16512]\n","loss: 0.887516  [14144/16512]\n","loss: 0.742211  [15424/16512]\n","Avg test loss: 0.659070 \n","\n","Epoch 7\n","-------------------------------\n","loss: 0.575715  [   64/16512]\n","loss: 2.137051  [ 1344/16512]\n","loss: 0.505829  [ 2624/16512]\n","loss: 0.577783  [ 3904/16512]\n","loss: 0.624667  [ 5184/16512]\n","loss: 0.587204  [ 6464/16512]\n","loss: 0.561592  [ 7744/16512]\n","loss: 0.713388  [ 9024/16512]\n","loss: 0.463486  [10304/16512]\n","loss: 0.544675  [11584/16512]\n","loss: 0.684041  [12864/16512]\n","loss: 0.524583  [14144/16512]\n","loss: 0.901176  [15424/16512]\n","Avg test loss: 0.640759 \n","\n","Epoch 8\n","-------------------------------\n","loss: 0.978286  [   64/16512]\n","loss: 0.433884  [ 1344/16512]\n","loss: 0.555921  [ 2624/16512]\n","loss: 0.413960  [ 3904/16512]\n","loss: 0.574470  [ 5184/16512]\n","loss: 0.688747  [ 6464/16512]\n","loss: 0.810608  [ 7744/16512]\n","loss: 0.482024  [ 9024/16512]\n","loss: 0.615549  [10304/16512]\n","loss: 0.458166  [11584/16512]\n","loss: 0.357224  [12864/16512]\n","loss: 0.559338  [14144/16512]\n","loss: 0.634889  [15424/16512]\n","Avg test loss: 0.624757 \n","\n","Epoch 9\n","-------------------------------\n","loss: 0.389839  [   64/16512]\n","loss: 0.276001  [ 1344/16512]\n","loss: 0.545917  [ 2624/16512]\n","loss: 0.773263  [ 3904/16512]\n","loss: 0.666093  [ 5184/16512]\n","loss: 0.437237  [ 6464/16512]\n","loss: 0.448183  [ 7744/16512]\n","loss: 0.592634  [ 9024/16512]\n","loss: 0.573349  [10304/16512]\n","loss: 0.705877  [11584/16512]\n","loss: 0.457673  [12864/16512]\n","loss: 0.590709  [14144/16512]\n","loss: 1.075918  [15424/16512]\n","Avg test loss: 0.610494 \n","\n","Epoch 10\n","-------------------------------\n","loss: 0.671805  [   64/16512]\n","loss: 0.486344  [ 1344/16512]\n","loss: 0.913702  [ 2624/16512]\n","loss: 0.430885  [ 3904/16512]\n","loss: 0.271527  [ 5184/16512]\n","loss: 0.671579  [ 6464/16512]\n","loss: 0.483162  [ 7744/16512]\n","loss: 0.602733  [ 9024/16512]\n","loss: 0.498304  [10304/16512]\n","loss: 1.595575  [11584/16512]\n","loss: 0.819862  [12864/16512]\n","loss: 0.484312  [14144/16512]\n","loss: 0.295451  [15424/16512]\n","Avg test loss: 0.597557 \n","\n","Done!\n"]}],"source":["device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n","\n","model = NeuralNetwork().to(device)\n","loss_fn = nn.MSELoss()\n","optimizer = optim.SGD(model.parameters(), lr=1e-3)\n","\n","num_epochs = 10\n","for t in range(num_epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(train_dataloader, model, loss_fn, optimizer, device)\n","    test_loop(test_dataloader, model, loss_fn, device)\n","print(\"Done!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BUJNJcdgFmS3"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":0}
