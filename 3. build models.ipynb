{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"jpM9s_hVYQmE"},"source":["This notebook is adapted from the official PyTorch tutorial on [Build Model](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"c-T5JEAAYBRg"},"source":["# Build the Neural Network\n","\n","Neural networks comprise of layers/modules that perform operations on data.\n","The [torch.nn](https://pytorch.org/docs/stable/nn.html) namespace provides all the building blocks you need to\n","build your own neural network. Every module in PyTorch is a subclass of [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n","A neural network is a module itself that consists of other modules (layers). This nested structure allows for\n","building and managing complex architectures easily.\n","\n","In the following sections, we'll build a neural network to predict median house values in the [California Housing](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset) dataset.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4347,"status":"ok","timestamp":1670616801763,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"D6hjZFIaYBRh"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","torch.set_printoptions(sci_mode=False, linewidth=300)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"dl-ut6tMYBRh"},"source":["## Load data\n","\n","Let's first load the data into a `DataLoader` as a review of previous tutorials. \n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3018,"status":"ok","timestamp":1670616836384,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"qLqBV6vHYBRi"},"outputs":[],"source":["from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","\n","X, y = fetch_california_housing(return_X_y=True)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":109,"status":"ok","timestamp":1670616920618,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"4QraED2taPX5"},"outputs":[],"source":["from torch.utils.data import TensorDataset\n","\n","X_train = torch.as_tensor(X_train, dtype=torch.float) # an alternative to torch.from_numpy\n","y_train = torch.as_tensor(y_train, dtype=torch.float)\n","X_test = torch.as_tensor(X_test, dtype=torch.float)\n","y_test = torch.as_tensor(y_test, dtype=torch.float)\n","\n","train_dataset = TensorDataset(X_train, y_train)\n","test_dataset = TensorDataset(X_test, y_test)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vDJlrb_xYBRi"},"source":["## Define the Class\n","We define our neural network by subclassing ``nn.Module``, and\n","initialize the neural network layers in ``__init__``. Every ``nn.Module`` subclass implements\n","the operations on input data in the ``forward`` method. \n","\n","In other words, the ``forward`` method should describe how input data will interact with your layers. \n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":136,"status":"ok","timestamp":1670617479803,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"OGA-4gAZALmy"},"outputs":[],"source":["class NeuralNetwork_v1(nn.Module):\n","    def __init__(self):\n","        super(NeuralNetwork_v1, self).__init__() # initialize the parent class \n","\n","        self.fc1   = nn.Linear(in_features=8, out_features=16)  # in_features is the data dim, 8\n","        self.relu1 = nn.ReLU()\n","        self.fc2   = nn.Linear(in_features=16, out_features=16) # In middle layers, in_features must match the last out_features\n","        self.relu2 = nn.ReLU()\n","        self.fc3   = nn.Linear(in_features=16, out_features=1)  # out_features is the label dim, 1\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.relu1(x)\n","        x = self.fc2(x)\n","        x = self.relu2(x)\n","        x = self.fc3(x)\n","        return x"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Amss2x0RYBRj"},"source":["We create an instance of ``NeuralNetwork_v1``, and move it to ``device``, and print\n","its structure.\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":534,"status":"ok","timestamp":1670617511681,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"ca8JGfmjYBRj","outputId":"51e935da-5c4f-4b10-ef14-cc2df3a18fab"},"outputs":[{"name":"stdout","output_type":"stream","text":["NeuralNetwork_v1(\n","  (fc1): Linear(in_features=8, out_features=16, bias=True)\n","  (relu1): ReLU()\n","  (fc2): Linear(in_features=16, out_features=16, bias=True)\n","  (relu2): ReLU()\n","  (fc3): Linear(in_features=16, out_features=1, bias=True)\n",")\n"]}],"source":["device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n","model_v1 = NeuralNetwork_v1().to(device) # .to method can also move an entire model to device\n","print(model_v1)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XjY9GDPaYBRj"},"source":["To use the model, we pass it the input data. This executes the model's ``forward``,\n","along with some [background operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866).\n","Do not call ``model.forward()`` directly!\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":117,"status":"ok","timestamp":1670617626313,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"sFHnUOIPYBRk","outputId":"5e24a62d-da58-41b4-ec15-4ae72c9280ae"},"outputs":[{"data":{"text/plain":["torch.Size([64, 1])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["inputs, labels = next(iter(train_dataloader))\n","inputs = inputs.to(device) # data must reside on the same device\n","labels = labels.to(device)\n","\n","outputs = model_v1(inputs) # call your model as if it were a function\n","outputs.size()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Qat3Pi85YBRk"},"source":["## Model Layers\n","\n","Let's break down the layers in the ``NeuralNetwork_v1`` model. To illustrate it, we\n","will take a sample minibatch of 3 data vectors of size 8 and see what happens to it as\n","we pass it through the network.\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":135,"status":"ok","timestamp":1670617706549,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"y9wmiyGfYBRk","outputId":"35e333a9-025b-4292-fc00-94727ad4ad21"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 8])\n"]}],"source":["x = inputs[:3]\n","print(x.size())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yuAzjvYlYBRl"},"source":["### nn.Linear\n","The [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n","is a module that applies a linear transformation on the input using its stored weights and biases.\n","\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":126,"status":"ok","timestamp":1670617789913,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"_mkUuDoZYBRl","outputId":"6bf6e4b3-87c2-41b6-b508-5ffca70f67bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 16])\n"]}],"source":["fc1_out = model_v1.fc1(x) # nn.Linear(in_features=8, out_features=16)\n","print(fc1_out.size())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"nq9ELq3UPwRj"},"source":["Note that the batch size `3` doesn't change, but the size of the data dimension has changed to `16` because `out_features` is 16. "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9Ero6B1FYBRl"},"source":["### nn.ReLU\n","Non-linear activations are what create the complex mappings between the model's inputs and outputs.\n","They are applied after linear transformations to introduce *nonlinearity*, helping neural networks\n","learn a wide variety of phenomena.\n","\n","In this model, we use [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) between our\n","linear layers, but there are [other activations](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) to introduce non-linearity in your model.\n","\n","No activation functions will change the **shape** of their input. \n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":340,"status":"ok","timestamp":1670617927894,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"EqcyJ7jJYBRl","outputId":"43b6d6c0-2a65-4a16-91cf-b27b4f55afeb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Before ReLU: tensor([[-371.4051, -145.0464, -250.5891,  353.9657, -182.2622,  147.7344,  -92.1813,  125.9600,  396.1821,  422.5164,  353.0605, -182.8116, -252.1051, -340.7018, -361.8319,  -93.8040],\n","        [-458.4600, -187.4800, -311.7845,  440.8052, -234.7249,  181.6513, -112.1790,  150.2557,  495.9763,  525.2939,  434.9780, -220.7425, -310.0343, -433.5242, -449.1116, -110.9794],\n","        [-298.5299, -109.0202, -196.1847,  284.0250, -138.7991,  117.8502,  -78.7035,  105.9789,  313.1189,  334.5900,  286.1516, -152.1156, -205.4304, -263.7532, -290.8603,  -79.3804]], device='mps:0', grad_fn=<LinearBackward0>)\n","\n","\n","After ReLU: tensor([[  0.0000,   0.0000,   0.0000, 353.9657,   0.0000, 147.7344,   0.0000, 125.9600, 396.1821, 422.5164, 353.0605,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n","        [  0.0000,   0.0000,   0.0000, 440.8052,   0.0000, 181.6513,   0.0000, 150.2557, 495.9763, 525.2939, 434.9780,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n","        [  0.0000,   0.0000,   0.0000, 284.0250,   0.0000, 117.8502,   0.0000, 105.9789, 313.1189, 334.5900, 286.1516,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000]], device='mps:0', grad_fn=<ReluBackward0>)\n"]}],"source":["print(f\"Before ReLU: {fc1_out}\\n\\n\")\n","relu1_out = model_v1.relu1(fc1_out) # nn.ReLU()\n","print(f\"After ReLU: {relu1_out}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RUFJTq26YBRl"},"source":["### New module: nn.Sequential\n","[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is an ordered\n","container of modules. The data is passed through all the modules in the same order as defined. We can define a single `nn.Sequential` container in place of the five individual layers in `NeuralNetwork_v1`. \n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":138,"status":"ok","timestamp":1670618158208,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"aolyr57lYBRi"},"outputs":[],"source":["class NeuralNetwork_v2(nn.Module):\n","    def __init__(self):\n","        super(NeuralNetwork_v2, self).__init__() # initialize the parent class \n","\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(in_features=8, out_features=16),  # in_features is the data dim, 8\n","            nn.ReLU(),\n","            nn.Linear(in_features=16, out_features=16), # In middle layers, in_features must match the last out_features\n","            nn.ReLU(),\n","            nn.Linear(in_features=16, out_features=1),  # out_features is the label dim, 1\n","        )\n","\n","    def forward(self, x):\n","        return self.linear_relu_stack(x) # no need to pass x around anymore; modules are called sequentially"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":141,"status":"ok","timestamp":1670618199330,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"ju2c5teIYBRm","outputId":"6d0db865-a773-4d0a-fd5e-52d93821c395"},"outputs":[{"data":{"text/plain":["torch.Size([64, 1])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["model_v2 = NeuralNetwork_v2().to(device)\n","outputs = model_v2(inputs) # call your model as if it were a function\n","outputs.size() # get outputs of the same size as that of model_v1"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lgiYkEqjTkZY"},"source":["### ReLU as a function\n","\n","Most activation functions do not have learnable parameters (one exception: [nn.PReLU](https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html#prelu), **P**arameterized **ReLU**). Therefore, they can be applied just like an ordinary `torch.*` function. "]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":173,"status":"ok","timestamp":1670618497047,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"G6Ki8o1NVH6A"},"outputs":[],"source":["class NeuralNetwork_v3(nn.Module):\n","    def __init__(self):\n","        super(NeuralNetwork_v3, self).__init__() # initialize the parent class \n","\n","        self.fc1 = nn.Linear(in_features=8, out_features=16)  # in_features is the data dim, 8\n","        self.fc2 = nn.Linear(in_features=16, out_features=16) # In middle layers, in_features must match the last out_features\n","        self.fc3 = nn.Linear(in_features=16, out_features=1)  # out_features is the label dim, 1\n","\n","        # homework: use nn.ModuleList to hold the three layers\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.fc2(torch.relu(x)) # ReLU now is no longer a \"layer\", but just a function\n","        x = self.fc3(torch.relu(x))\n","        return x"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":151,"status":"ok","timestamp":1670618501079,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"Bg31ztsgWrlb","outputId":"9219294b-9e09-4bde-b422-68c0c1f65c0c"},"outputs":[{"data":{"text/plain":["torch.Size([64, 1])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["model_v3 = NeuralNetwork_v3().to(device)\n","outputs = model_v3(inputs) # call your model as if it were a function\n","outputs.size() # get outputs of the same size as that of model_v1"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"k7OwBkhBWyO1"},"source":["If your activation function is not listed as `torch.*`, you should be able to find it under [torch.nn.functional](https://pytorch.org/docs/stable/nn.functional.html). "]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":166,"status":"ok","timestamp":1670618579535,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"MdLkprUAXAC4","outputId":"e0e7d119-1d2f-47e8-88f8-b5a325041b0f"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["import torch.nn.functional as F\n","\n","torch.allclose(F.relu(fc1_out), torch.relu(fc1_out))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZsyRm3I9YBRm"},"source":["## Model Parameters\n","Many layers inside a neural network are *parameterized*, i.e. have associated weights\n","and biases that are optimized during training. Subclassing ``nn.Module`` automatically\n","tracks all fields defined inside your model object, and makes all parameters\n","accessible using your model's ``parameters()`` or ``named_parameters()`` methods.\n","\n","In this example, we iterate over each parameter, and print its size and a preview of its values.\n","\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":338,"status":"ok","timestamp":1670618712224,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"bDDC8ObPYBRm","outputId":"9fa8892d-8e8b-443f-a8d1-5015100ed6a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model structure: NeuralNetwork_v1(\n","  (fc1): Linear(in_features=8, out_features=16, bias=True)\n","  (relu1): ReLU()\n","  (fc2): Linear(in_features=16, out_features=16, bias=True)\n","  (relu2): ReLU()\n","  (fc3): Linear(in_features=16, out_features=1, bias=True)\n",")\n","\n","\n","Layer: fc1.weight | Size: torch.Size([16, 8]) | Values : tensor([[-0.0577, -0.2311,  0.3286,  0.0309, -0.2904,  0.2855, -0.1345,  0.2278],\n","        [ 0.0154,  0.0417, -0.1681,  0.2118, -0.1450, -0.2586,  0.1913, -0.1318]], device='mps:0', grad_fn=<SliceBackward0>) \n","\n","Layer: fc1.bias | Size: torch.Size([16]) | Values : tensor([ 0.0975, -0.0100], device='mps:0', grad_fn=<SliceBackward0>) \n","\n","Layer: fc2.weight | Size: torch.Size([16, 16]) | Values : tensor([[ 0.1630,  0.2013,  0.2361, -0.2312, -0.0823, -0.0157, -0.0339,  0.2347,  0.0601,  0.1327,  0.2467, -0.2486,  0.2320,  0.0461,  0.0623,  0.2198],\n","        [ 0.1207,  0.0952, -0.1620,  0.1489,  0.2330,  0.2324, -0.1856, -0.0587, -0.0917, -0.1555, -0.1636,  0.0731,  0.1939,  0.2136,  0.0974,  0.0147]], device='mps:0', grad_fn=<SliceBackward0>) \n","\n","Layer: fc2.bias | Size: torch.Size([16]) | Values : tensor([-0.2375, -0.1348], device='mps:0', grad_fn=<SliceBackward0>) \n","\n","Layer: fc3.weight | Size: torch.Size([1, 16]) | Values : tensor([[-0.2049,  0.0706,  0.1452, -0.0265,  0.0674, -0.2330,  0.1276, -0.1473,  0.1694,  0.1535, -0.1471, -0.0041,  0.2398,  0.1384,  0.0374,  0.1744]], device='mps:0', grad_fn=<SliceBackward0>) \n","\n","Layer: fc3.bias | Size: torch.Size([1]) | Values : tensor([0.1292], device='mps:0', grad_fn=<SliceBackward0>) \n","\n"]}],"source":["print(f\"Model structure: {model_v1}\\n\\n\")\n","\n","for name, param in model_v1.named_parameters():\n","    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0QndnruXYBRm"},"source":["--------------\n","\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":0}
