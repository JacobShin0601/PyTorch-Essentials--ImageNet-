{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"TKV34_aUB34W"},"source":["This notebook is adapted from the official PyTorch tutorial on [Datasets & DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"l4CRlZK2Bud3"},"source":["# Datasets & DataLoaders\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mfsyKo42Bud4"},"source":["Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code\n","to be decoupled from our model training code for better readability and modularity.\n","\n","PyTorch provides two data primitives: ``torch.utils.data.DataLoader`` and ``torch.utils.data.Dataset``\n","that allow you to use pre-loaded datasets as well as your own data.\n","\n","``Dataset`` stores the samples and their corresponding labels, and ``DataLoader`` wraps an iterable around\n","the ``Dataset`` to enable easy access to the samples.\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"PZ11ny4qC1aG"},"source":["PyTorch domain libraries provide a number of pre-loaded datasets (such as FashionMNIST) that\n","subclass ``torch.utils.data.Dataset`` and implement functions specific to the particular data.\n","They can be used to prototype and benchmark your model. You can find them\n","here: [Image Datasets](https://pytorch.org/vision/stable/datasets.html),\n","[Text Datasets](https://pytorch.org/text/stable/datasets.html), and\n","[Audio Datasets](https://pytorch.org/audio/stable/datasets.html)."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"icMhHx2JCjmq"},"source":["In this tutorial though, we will learn how to load your own data that is not part of a PyTorch official dataset. \n","\n","We will use the classic [California Housing](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset) dataset as a running example. "]},{"cell_type":"code","execution_count":1,"metadata":{"id":"r9O0thB3DQuo"},"outputs":[],"source":["from sklearn.datasets import fetch_california_housing\n","\n","X, y = fetch_california_housing(return_X_y=True, as_frame=True)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":262,"status":"ok","timestamp":1669913082593,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"31DVnaJODuNX","outputId":"a61fa592-8f9c-4d7d-d769-bfe20ab2a384"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>MedInc</th>\n","      <th>HouseAge</th>\n","      <th>AveRooms</th>\n","      <th>AveBedrms</th>\n","      <th>Population</th>\n","      <th>AveOccup</th>\n","      <th>Latitude</th>\n","      <th>Longitude</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>8.3252</td>\n","      <td>41.0</td>\n","      <td>6.984127</td>\n","      <td>1.023810</td>\n","      <td>322.0</td>\n","      <td>2.555556</td>\n","      <td>37.88</td>\n","      <td>-122.23</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>8.3014</td>\n","      <td>21.0</td>\n","      <td>6.238137</td>\n","      <td>0.971880</td>\n","      <td>2401.0</td>\n","      <td>2.109842</td>\n","      <td>37.86</td>\n","      <td>-122.22</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>7.2574</td>\n","      <td>52.0</td>\n","      <td>8.288136</td>\n","      <td>1.073446</td>\n","      <td>496.0</td>\n","      <td>2.802260</td>\n","      <td>37.85</td>\n","      <td>-122.24</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5.6431</td>\n","      <td>52.0</td>\n","      <td>5.817352</td>\n","      <td>1.073059</td>\n","      <td>558.0</td>\n","      <td>2.547945</td>\n","      <td>37.85</td>\n","      <td>-122.25</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3.8462</td>\n","      <td>52.0</td>\n","      <td>6.281853</td>\n","      <td>1.081081</td>\n","      <td>565.0</td>\n","      <td>2.181467</td>\n","      <td>37.85</td>\n","      <td>-122.25</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n","0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n","1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n","2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n","3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n","4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n","\n","   Longitude  \n","0    -122.23  \n","1    -122.22  \n","2    -122.24  \n","3    -122.25  \n","4    -122.25  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["X.head()"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":309,"status":"ok","timestamp":1669913112032,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"wg5avysMESq0","outputId":"6309cdbc-2896-4c69-c625-a25220e7c96a"},"outputs":[{"data":{"text/plain":["0    4.526\n","1    3.585\n","2    3.521\n","3    3.413\n","4    3.422\n","Name: MedHouseVal, dtype: float64"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["y.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HAQUodK1FHpT"},"source":["We are now in the deep-learning realm, so let's cast everything into PyTorch tensors!"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ERLH1seFEy_5"},"outputs":[],"source":["import torch\n","\n","torch.set_printoptions(sci_mode=False, linewidth=300)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"xVR7S2XxEcYN"},"outputs":[],"source":["X = torch.from_numpy(X.to_numpy()).float()\n","y = torch.from_numpy(y.to_numpy()).float()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":202,"status":"ok","timestamp":1669913221520,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"xuKuX0kLF91t","outputId":"fca46c94-0e68-48bd-95da-bfdaf206c250"},"outputs":[{"data":{"text/plain":["(torch.Size([20640, 8]), torch.Size([20640]))"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["X.size(), y.size()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_BAIqLj9JcIh"},"source":["A note on `dtype`: In NumPy, the default `dtype` is usually `np.float64` (also known as the `double` type). But in PyTorch the default is `torch.float32` (also known as the `float` type). Computations performed in double precisions are often much slower and more memory-intensive than that in single precisions, even on a GPU. Moreover, the additional precision offered by `double` usually doesn't matter (i.e., it won't affect the evaluation metrics). That's why we cast `X` and `y` to `torch.float32` using `.float()`. "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"b7mpFaaFL46s"},"source":["## Creating a Custom Dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"99EWrfsHGEIn"},"source":["When your data is already stored as tensors, [TensorDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) will come handy. "]},{"cell_type":"code","execution_count":7,"metadata":{"id":"rLMFcPkUGDjl"},"outputs":[],"source":["from torch.utils.data import TensorDataset\n","\n","cal_housing = TensorDataset(X, y) # simply pass your tensors here"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"q1-ZYe0YG7Ik"},"source":["OK, but why do we need a `Dataset`? Because it allows us to index into your tensors and retrieve (features, label) pairs. "]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":192,"status":"ok","timestamp":1669913508187,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"O3l1W4mUGuch","outputId":"11c81619-10fb-44a3-fb54-cb22aa81bf94"},"outputs":[{"data":{"text/plain":["(tensor([   8.3252,   41.0000,    6.9841,    1.0238,  322.0000,    2.5556,   37.8800, -122.2300]),\n"," tensor(4.5260))"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["cal_housing[0] # a tuple of (features, label)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":192,"status":"ok","timestamp":1669913546944,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"HtA9yeU6HpG-","outputId":"8b54b5df-384a-4a92-fbf9-ff1957309d3f"},"outputs":[{"data":{"text/plain":["(tensor([[     8.3252,     41.0000,      6.9841,      1.0238,    322.0000,      2.5556,     37.8800,   -122.2300],\n","         [     8.3014,     21.0000,      6.2381,      0.9719,   2401.0000,      2.1098,     37.8600,   -122.2200],\n","         [     7.2574,     52.0000,      8.2881,      1.0734,    496.0000,      2.8023,     37.8500,   -122.2400],\n","         [     5.6431,     52.0000,      5.8174,      1.0731,    558.0000,      2.5479,     37.8500,   -122.2500],\n","         [     3.8462,     52.0000,      6.2819,      1.0811,    565.0000,      2.1815,     37.8500,   -122.2500]]),\n"," tensor([4.5260, 3.5850, 3.5210, 3.4130, 3.4220]))"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["cal_housing[:5] # a tuple of (the first 5 rows, the first 5 labels). "]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":193,"status":"ok","timestamp":1669913597993,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"fHKQSoD5IQio","outputId":"11857b08-2c7a-4612-e74a-e00adc802da5"},"outputs":[{"data":{"text/plain":["20640"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["len(cal_housing) # the length of our dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3QIzSIxUM5VZ"},"source":["We can split our dataset into training and testing sets using PyTorch's `random_split` function. \n","\n","You may also split the data beforehand using your favourite package and create separate `Dataset` for each split. But, when in Rome..."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"joI-zluwNFzS"},"outputs":[],"source":["import math\n","from torch.utils.data import random_split\n","\n","train_frac = 0.7\n","train_size = math.floor(train_frac * len(cal_housing))\n","test_size = len(cal_housing) - train_size\n","\n","cal_housing_train, cal_housing_test = random_split(cal_housing, (train_size, test_size))"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":199,"status":"ok","timestamp":1669913755541,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"bi4xaOaUNWKw","outputId":"cb2f27e3-94f8-47b1-f235-199ff689a458"},"outputs":[{"data":{"text/plain":["(14447, 6193)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["len(cal_housing_train), len(cal_housing_test)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"wsqAJJqqBud8"},"source":["## Preparing your data for training with DataLoaders\n","The ``Dataset`` retrieves our dataset's features and labels one sample at a time. While training a model, we typically want to\n","pass samples in \"minibatches\", reshuffle the data at every epoch to reduce model overfitting, and use Python's ``multiprocessing`` to\n","speed up data retrieval.\n","\n","[DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) is an iterable that abstracts this complexity for us in an easy API.\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"YmhtXGMLBud8"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","train_dataloader = DataLoader(cal_housing_train, batch_size=64, shuffle=True)\n","test_dataloader = DataLoader(cal_housing_test, batch_size=64, shuffle=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"SZCWPNQqBud8"},"source":["## Iterate through the DataLoader\n","\n","We have loaded that dataset into the ``DataLoader`` and can iterate through the dataset as needed.\n","Each iteration below returns a batch of ``train_features`` and ``train_labels`` (containing ``batch_size=64`` features and labels respectively).\n","Because we specified ``shuffle=True``, after we iterate over all batches the data is shuffled (for finer-grained control over\n","the data loading order, take a look at [Samplers](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler)).\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":230,"status":"ok","timestamp":1669914049318,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"CwZTGibfBud8","outputId":"8c0fc4b0-32d0-43db-fff3-918cf58027e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n"]}],"source":["train_features, train_labels = next(iter(train_dataloader))\n","\n","print(f\"Feature batch shape: {train_features.size()}\")\n","print(f\"Labels batch shape: {train_labels.size()}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_Qd83nHVQf3H"},"source":["`next` + `iter` is useful when you want to examine a batch of data (e.g., to check if you have implemented your data-loading functions correctly). \n","\n","But a more common paradigm is to use your `DataLoader` with a `for` loop, so that you can continuously step into your batches. "]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":416,"status":"ok","timestamp":1669914229882,"user":{"displayName":"Yumou Wei","userId":"07928071509313005295"},"user_tz":300},"id":"r2Kbo8e-RwnY","outputId":"dd4e98f6-6b30-4515-cb6a-f57d06d02dee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([64, 8])\n","Labels batch shape: torch.Size([64])\n","Feature batch shape: torch.Size([49, 8])\n","Labels batch shape: torch.Size([49])\n"]}],"source":["for features, labels in test_dataloader:\n","    print(f\"Feature batch shape: {features.size()}\")\n","    print(f\"Labels batch shape: {labels.size()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jV3BA_ckpPlL"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":0}
